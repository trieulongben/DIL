# -*- coding: utf-8 -*-
"""Linear Regression Div.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wxByQGxq0SJ5wlmV488X4M-RiBXq_coU
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision
import torchvision.transforms as transforms
from sklearn.datasets import load_boston
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader

X,Y= load_boston(return_X_y=True)

x_train,x_test,y_train,y_test=train_test_split(X,Y)

def normalize(x):
  mean=x.mean(axis=0)
  std=x.std(axis=0)
  return (x-mean/std)
X,Y= load_boston(return_X_y=True)
print(X[0])
X=normalize(X)
print(X[0])

y_train=y_train.reshape(379,1)

import os
import pandas as pd
from torchvision.io import read_image
from torch.utils.data import Dataset

class bostonData(Dataset):
    def __init__(self):
        X,Y= load_boston(return_X_y=True)
        print(X[0])
        X=self.normalize(X)
        x_train,x_test,y_train,y_test=train_test_split(X,Y)
        self.x,self.y = x_train,y_train
        print(X[0])
    def normalize(self,x):
      mean=x.mean(axis=0)
      std=x.std(axis=0)
      return (x-mean/std)
    def __len__(self):
        return len(self.x)

    def __getitem__(self, idx):
        return self.x[idx], self.y[idx]

next(iter(x))

x_train.shape[1]

"""XW+b"""

class LinearRegression():
  def __init__(self):
    self.w=1
    self.b=0
      
  def train(self,batch,lr,num_epocs):
    dataset=DataLoader(bostonData(), batch_size=batch, shuffle=True)
    self.w=torch.randn(x_train.shape[1],1, requires_grad=True,dtype=torch.double)
    self.b=torch.randn(1,requires_grad=True,dtype=torch.double)
    #SGD
    for epoc in range(num_epocs):
      x,y=next(iter(dataset))
      z=self.obFunc(x)
      loss=self.loss()
      loss(z,y).sum().backward()
      self.updateParams([self.w,self.b],lr=lr,batch=batch)
      with torch.no_grad():
        train_loss=self.loss()(z,y)
        print(f'epoch {epoc + 1}, loss {float(train_loss.mean()):f}')
  def updateParams(self,params,lr,batch):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch
            param.grad.zero_()
  def loss(self):
    return torch.nn.MSELoss()
  def obFunc(self,x):
    return torch.matmul(x,self.w)+self.b
  def predict(self,x):
    return torch.matmul(torch.tensor(x,requires_grad=False),self.w)+self.b
  def test(self,x_test,y_test):
    import numpy as np
    return np.sqrt((self.predict(x_test).detach().numpy()-y_test.reshape(127,1))**2).sum()/127

model=LinearRegression()
model.train(batch=35,lr=0.0001,num_epocs=5000)

test_pred=model.predict(x_test)

y_test.shape

def test(self,x_test):
  import numpy as np
  return np.sqrt((test_pred.detach().numpy()-y_test.reshape(127,1))**2).sum()/127

from sklearn.linear_model import LinearRegression
regressor = LinearRegression(normalize=True)
regressor.fit(x_train, y_train)

from sklearn import metrics
y_pred = regressor.predict(x_train)
print('Mean Squared Error:', metrics.mean_squared_error(y_train, y_pred))

regressor.coef_

model.w